\documentclass[12pt, a4paper]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{vmargin}
\usepackage{amsthm}
\usepackage{graphicx}

\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem*{definition}{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\title{Representing Penalties on Basis Expansions as a Polynomial In the Parameters (Working Title)}

\begin{document}
\maketitle

\section{Basis Function Expansion Methods}

Fitting sophisticated mathematical functions to empirical data continues to be a challenge in statistical science. One approach, known as the basis function expansion method offers a considerable degree of flexibility and mathematically tractable solutions. This approach involves representing an unknown function $y(t)$ using a finite number of basis functions $\{\phi_k(t)\}$ through the conditional expectation

\[
	\mathbb{E}[y|t] = c_0\phi_0(t) + c_1\phi_1(t) + \dots + c_N\phi_N(t),
\]


\par\noindent where $\{c_k\}$ is a set of appropriately chosen coefficients. 

\smallskip

Here, $\mathbb{E}[y|t]$ is the conditional expectation of the dependent variable $y$, given our independent data $x$. The conditional expectation $\mathbb{E}[y|t]$ is the estimate of $y$ depending exclusively on $t$ that minimises the squared loss $\mathbb{E}[y(t) - E[y|t]]^2$. This is a theoretical optimiser, and so in practice we must make do with an estimate $\widehat{\mathbb{E}[y|t]}$ instead.

\begin{example}(Simple Linear Regression) Simple linear regression is an example of a statistical procedure that uses a basis function representation. Take two basis functions $\{1, t\}$, so that $\hat{y}(t)$, our estimate of $y$ given the value $t$, can be written as a linear combination of the two $\widehat{\mathbb{E}[y|t]} = \hat{y}(t) = c_0 + c_1t.$

This is the form of a simple linear regression model. If our basis consists of just the single constant function $\phi(t) = 1$ then we get a model of the form $\hat{y} = c_0.$

\smallskip

In this case we would generally use the mean of the $y$ values as our estimate of $c_0$. If we go in the other direction and add a quadratic function $t^2$ we get a quadratic regression model $\hat{y}(t) = c_0 + c_1t + c_2t^2.$

\qedsymbol
\end{example}

\begin{example}(Broken Stick Function)  Another choice of basis is the two functions $\{\phi_1, \phi_2\}$ defined as follows

\begin{align*}
	\phi_1(t) &= 1 \\
	\phi_2(t) &= 
		\begin{cases}
			0 & \text{if } t < \tau \\
                 t - \tau & \text{if } t \geq \tau.
		\end{cases} 
\end{align*}

\par\noindent The first function is a constant function, the second is a ramp function based at $\tau$.

\begin{figure}[t]
	\centering
	\includegraphics[scale = 0.5]{ramp_function.pdf}
	\caption{The broken stick function  $\phi_2.$}
	\label{fig:ramp_function}
\end{figure}

\smallskip

This basis would work very well for representing data which starts to increase linearly past a certain threshold.  Broken stick functions and constant functions are very cheap computationally, only needing literally a line or two of code to implement and only have at most one associated parameter $(\tau)$, consuming little memory.  A computer can produce and work with comparatively a very large number of them at once. This could be used to cover situations where there data is unchanging over relatively long periods.

\smallskip

Ramp functions are the building blocks for the ``p-splines'' developed by Eiler and Marx. P-splines are a precursor to the penalty based approach used here. P-splines penalise the coefficients, whereas we will penalise the generated curve.

\smallskip

The above basis has two disadvantages though. Firstly the second basis function is only continuous, not differentiable. It would not be wise to use this basis if we wanted to estimate $\mathrm{d}y/\mathrm{d}t$. Secondly it is very arbitrary. It is not obvious why it would be useful compared to a quadratic model for example; the latter also allows to us estimate derivatives of all orders and can still accommodate data which varies in its rate of change.

\qedsymbol
\end{example}

This underscores an important point. There are many types of basis besides the ones usually encountered, such as polynomial bases. Many of them are fungible from the pure mathematical point of view in terms of how well they can fit a function. The statistician should make a sensible choice of basis. If our choice of basis is good, then it will be able to fit the data with only a few terms, and we might be able to avoid estimating many coefficients. This is one of the reasons why simple linear regression and quadratic regression are useful; they can capture much variation in the observed data in spite of being very simple.

\begin{example}(Fourier Basis Functions)
If our data has a periodic component to it, such as the observed temperature over the course of the year, or a time series of annual sales, then it seems intuitively correct to use a basis consisting of periodic functions. This suggests that we should use a Fourier basis consisting of the set of functions $\{\cos(n\omega t), \sin(n\omega t)\}$ where $0 \leq n \leq N$ for some $N$ and $\omega$ is the frequency. The frequency depends on our time scale, they are related by the formula $\omega = 2\pi/T$, where $T$ is the period. 

\smallskip

Furthermore Fourier basis functions have several other desirable properties. They are smooth, meaning that they can be used to estimate any derivative of the data, at least in theory. They are orthogonal, which can make certain problems more convenient and they are closed under differentiation, meaning that the derivative of a combination of Fourier basis functions, is itself a combination of Fourier basis functions. The latter two properties will prove very useful later, as we shall see. 

\smallskip

A Fourier basis can represent  any square integrable function on some interval. This covers a large proportion of the functions encountered in real life  They cannot detect how frequencies change in space however, only their global behaviour. They can also be somewhat mundane - almost certain to work, but unlikely to provide anything too interesting.

\smallskip
\qedsymbol
\end{example}
\begin{example}(B-spline Basis Functions) Roughly speaking, B-splines are compactly supported polynomial functions, or more practically they are nonzero only inside of a given interval. More formally a B-Spline basis consists of a degree $n$, which determines the degree of the basis functions, and a set of knots, that is a set of $K$ time points $\{t_0, \dots, t_K\}$. 

\smallskip

Since they are compact they generally can only individually capture local information about the data. One of the main advantages of B Splines is that they can represent any other spline of the same degree and smoothness with the same knots. This makes them useful for statistics since make fewer assumptions less about the form of the data, they can help us avoid bias.

\qedsymbol
\end{example}

\section{Least Squares Fitting}

Least squares fitting is a means of fitting a function to data. The least squares fit is the one that minimises the sums of the squared errors.  In the cases where we have $n$ observations $y_i$ measured at times $t_i$ and where we are selecting our estimated function $\hat{y}(t)$ from a set of functions $\mathit{S}$, the least squares fit $\hat{y}_{LS}(t)$ is defined as

\[
	\hat{y}_{LS}(t) = \underset{f \in \mathit{S}}{\operatorname{argmin}}\sum_{i = 1}^n [y_i - f(t_i)]^2.
\]

\par\noindent From now on we will always be working with the sums of the squared errors, so we will not bother using any subscripts to indicate this and only use $\hat{y}(t)$ rather than $\hat{y}_{LS}(t).$

\smallskip

Assume $\mathit{S}$ can be spanned by some set of $m$ basis functions i.e. $\mathit{S} = \{\sum\limits_{i = 1}^mc_i\phi_i(t)| c_i \in \mathbb{R}\}$. This suggests that to find $\hat{y}(t)$ it is only necessary to estimate to coefficients $\hat{c}_i$, so that $\hat{y}(t)$ never appears explicitly. Then we have completely determined $\hat{y}$. We can then write the least squares criterion as:

\[
	(\hat{c}_1, \dots, \hat{c}_m)  = \underset{c_1 \dots c_K}{\operatorname{argmin}} \sum_{i=1}^n[y_i - \sum_{j=1}^m c_j\phi_j(t_i)]^2
\]

The above expression is can be expressed more cleanly using vector notation. Firstly, we  have $f(t) = \mathbf{c'\phi(t)}$, where $\mathbf{c} = (c_1,..., c_m)$ and $\mathbf{\phi} = (\phi_1(t), \dots, \phi_2(t))'$.  By constructing a matrix $\mathbf{\Phi}$, where the $i$th row of $\mathbf{\Phi}$ is $\mathbf{\phi}(t_i)$, and let $\mathbf{y} = (y_1, \dots, y_n)$ the least squares problem  can written as

\[
	\hat{\mathbf{c}} = \underset{\mathbf{c} \in \mathbb{R}^m}{\operatorname{argmin}} (\mathbf{y - \Phi c})'(\mathbf{y - \Phi c}).
\]

The expression on the right hand side is an example of a quadratic form, they are the generalisation of quadratic functions to finite dimensional vector spaces. We can have quadratic forms on infinite dimensional vectors spaces too, but that is not relevant here.

The advantage of quadratic forms is that they are very easy to minimise as any textbook on regression or numerical optimisation will tell you.
\section{Roughness Penalties}

It is well known that the least squares gives us the BLUE of $\mathbf{y},$ the function we assume to be generating the data. Nonetheless it is often useful to employ a form of regularisation which constrains how much $\hat{y}$ is allowed to vary. Intuitively, this reduces the variation of $\hat{y}$ and helps guard against overfitting. In the context of basis function expansions, the most commonly used penalty is the curvature penalty

\[
	\mathrm{PEN}(\hat{y}) = \int_\mathit{D}[\hat{y}''(t)]^2\mathrm{d}t.
\]

\par\noindent Here $\mathit{D}$ is our domain of interest and $\hat{y}''(t)$ stands for the second derivative of the function $\hat{y}(t).$ 

\smallskip

This penalty tries to force $\hat{y}(t)$ towards solutions of the differential equation $\hat{y}''(t) = 0$. The general solution of this ODE is of the form $\hat{y}(t) = \alpha + \beta t$, a straight line. The penalty nudges $\hat{y}(t)$ towards linear regression.

\smallskip

This penalty can also be represented as a polynomial. Let $\langle f,g \rangle = \int_\mathit{D}f(t)g(t)\mathrm{d}t.$ Here, $\langle \cdot, \cdot \rangle$ is the inner product on $\mathit{L}^2(\mathit{D})$, the set of square integrable functions on $\mathit{D}.$ The penalty can be written in the form $\mathrm{PEN}(f) = \langle f'', f''\rangle$. Substituting in the expansion for $f$ we get

\begin{align*}
	\langle f'', f'' \rangle &= \langle \sum_{i=1}^mc_i\phi_i'', \sum_{i=1}^mc_i\phi_i'' \rangle \\	
				 &= \sum_{i=1}^m\sum_{j=1}^m\langle c_i\phi_i'', c_j\phi_j'' \rangle \\ 
				 &= \sum_{i=1}^m\sum_{j=1}^mc_ic_j\langle \phi_i'', \phi_j'' \rangle.
\end{align*}

Here, $\langle \phi_i'', \phi_j'' \rangle$ depends only on our choice of basis and so are ``fixed''
for our purposes. This implies that the penalty can be represented as a polynomial in the $c_i$. We can do better however and represent the penalty as a quadratic form. If we define an $m \times m$ matrix $\mathbf{K}$ by $\mathbf{K}_{ij} = \langle \phi_i'', \phi_j'' \rangle$ it can be seen that $\langle f'', f'' \rangle = \mathbf{c'Kc}$.  

\smallskip

Note that this relies on the fact that the second derivative operator maps a linear combination of $\phi_i$ into a linear combination $\phi_i''$. We can always represent an inner product on some finite vector space as $\mathbf{b'Ab}$, where the terms depend on the problem at hand. We can simply construct the matrix $\mathbf{A}$ for the $\phi_i''$ and have the $\mathbf{c}$ in place of the $\mathbf{b}$ because differentiation is linear.


\section{Penalised Least Squares}

There are two distinct, but competing objectives. We want a fit $\hat{y}$ that maintains fidelity to the data, as represented by the goodness of fit, but simultaneously adheres to the requirement of a smooth description of the data. 

\smallskip

The solution is to use a penalty that incorporates both of these objectives, the penalised sum of squared errors. The penalised sum of squared errors is a functional, or a function whose domain is a set of functions and whose codomain is the real numbers. Here it is a functional on the spanning set of our basis functions $\{\phi_i(t)\}.$ It is the weighted sum of the roughness and least squares penalties

\[
	\mathrm{PENSSE}(f) = \sum_{i = 0}^n (y_i - f(t_i)^2 + \lambda \int_\mathit{D} [f''(t)]^2\mathrm{d}t.
\]

The parameter $\lambda$ controls the tradeoff between error and smoothness as represented by the least squares and roughness terms respectively. It can be thought of as a model complexity parameter; as $\lambda$ increases $\hat{y}(t)$ becomes more linear and so our model tends more towards simple linear regression, as it decreases the model tends more towards fitting the data exactly. 

\smallskip

In polynomial regression the analogous parameter would be the order of the polynomial we are fitting. However the two quantities differ in that the order of a polynomial is a discrete quantity, whilst $\lambda$ is a continuous one. 

\smallskip

\par\noindent We can also think of $\lambda$ as controlling the degree of fidelity of the fitted curve to the ODE $\hat{y}''(t) = 0$.

\smallskip

The choice of $\lambda$ is important; strictly speaking we ought to denote $\hat{y}(t)$ as $\hat{y}_{\lambda}(t)$ to denote the dependence. An estimate for $\lambda$ can generally be found by cross validation. As discussed in Green and Silverman sometimes there are better choices of $\lambda$ than a computer can find though.

\smallskip

Assume again that $\hat{y}(t)$ is the sum of basis functions, depending on a vector of coefficients $\mathbf{\hat{c}}$. This means we can omit the dependence on $\hat{y}(t)$ and convert from the problem of estimating a function to estimating a vector $\mathbf{\hat{c}(\lambda)}$ (or just simply $\mathbf{\hat{c}}$) which depends on our choice of smoothing parameter. Using the previous results we can show that the Penalised Sum of Squares can be written as the sum of two quadratic forms:

\[
	\mathrm{PENSSE}(c) = (\mathbf{y - \Phi c})'(\mathbf{y - \Phi c}) + \lambda \mathbf{c}'\mathbf{Kc}.
\]

Where $\mathbf{y}$, $\mathbf{K}$ and $\mathbf{\Phi}$ have the same definitions as they did before. This is an regularised regression problem or Tikhonov regularisation. The problem of minimising this expression has the solution 

\begin{align*}
	\mathbf{\hat{c}(\lambda)} &= \mathbf{(\Phi'\Phi + \lambda K)^{-1}\Phi'y} \\ 
			 &= \mathbf{H}(\lambda)\mathbf{y}
\end{align*}

If this problem is looked at from the perspective of linear models, as in Christessen, then $\mathbf{y} = \mathbf{\Phi c + e}$, where the elements of $\mathbf{e}$ are i.i.d and have mean zero and variance $\sigma^2$, then we can estimate the variance of $\hat{y}(t).$ We will denote the hat matrix $\mathbf{H}(\lambda)$ by $\mathbf{H}.$

We have $\mathbf{\hat{c}} = \mathbf{H(\Phi c + e)},$ so $\mathbb{E}[\mathbf{\hat{c}}] = \mathbf{H\Phi c}.$ Note $\mathbf{\hat{c}}$ can be a biased estimate of $\mathbf{c}$. The variance of $\mathbf{\hat{c}}$ is given by $\mathrm{Var}[\mathbf{\hat{c}}] = \sigma^2\mathbf{H\Phi\Phi' H'}.$ Hence the expected value of $\mathbf{\hat{y}}$ is $\mathbf{\Phi H\Phi c}$ and its variance is $\sigma^2 \mathbf{\Phi H\Phi\Phi' H'\Phi'}$.

\section{Multivariate Bases}

What if our data is spatial in nature? In this case our data will often be at a series of points $\mathbf{x}_i = (x_i, y_i); i = 1, \dots ,n$. It is actually not too difficult to extend our results; it is almost as easy as replacing the $t_i$ with $\mathbf{x_i}$.

\smallskip

As before we expand our function $y$ as a basis function expansion

\[
	y(\mathbf{x}) = \sum_{i=1}^m c_i\phi(\mathbf{x}).
\]

Notice that even though we are working in more than one dimension, our sum is still ``one dimensional'' in that it is taken over a single index. This is deliberate as it makes our life much easier.


\smallskip

Finding a least squares estimate is identical, so we will not cover it here.

\newpage

\begin{example}(Fitting the Displacement of a Membrane Using a Laplacian Penalty) Suppose we had measurements of protrusion of a plate or membrane and we wished to fit some functions to it. We would assume the surface is not moving, as would be the case for clingfilm over a bowl for example.  We could use standard interpolation or least squares methods. However the mechanics of these objects are often governed by Partial Differential Equations (PDEs). We would like to incorporate this information somehow. 
	
\smallskip

If the perturbation of the surface is small, then it can be shown that a model of the mechanics of surface is a PDE called the wave equation. Let $u(x,y,t)$ be the displacement of the membrane at position $(x,y)$ at time $t$. Then $u$ is assumed to satisfy the partial differential equation:

\[
	u_{tt} = c\Delta u.
\]

Here $u_{tt} = \partial^2u / \partial t^2$, $c$ is a parameter known as the wave speed and $\Delta$ is a differential operator known as the Laplacian. In two dimensions withCartesian coordinates the Laplacian is defined as  

\[
	\Delta = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2}.
\]

We are interested in the steady state model, where the membrane doesn't move so that $u_{tt} = 0.$ Of course $u$ now only depends on the position so $u = u(x,y).$  In this case the wave equation reduces to Laplace's equation

\[
	\Delta u = 0.
\]

\par\noindent Since the wave equation isn't completely valid and only an approximation, we should not expect the data adhere perfectly to Laplace's equation anymore  than we expect data to have a perfectly linear relationship. For example the surface could be bending too much for the wave equation to hold to high degree of accuracy. It is hoped that some use can be gotten out of the model though by using it in penalised regression with respect to the penalty function
\smallskip

\[
	\mathrm{PENSSE}(f) = \sum_{i=0}^n(u_i - f(\mathbf{x}_i))^2 + \lambda \int_{\Omega}|\Delta f(\mathbf{x})|^2 \mathbf{dx}.
\]

As usual we use $\hat{u}(x,y)$ to denote the function that minimises the penalty above that is then spanning set of our basis; it is an estimate.

\smallskip

The interpretation of the penalty in this case is a little more subtle than before. Harmonic functions, as solutions to Laplace's equation are called, are not flat as if they were linear, though linear functions are clearly harmonic. The laplacian penalty has more ``give'' than the roughness penalty in one dimension, so we should not expect it to necessarily approach a plane as $\lambda$ increases. 

 Generally $\lambda$ would be chosen by a cross-validation approach as was the case for the roughness penalty. It is interesting to speculate about the implications of the order of magnitude of $\lambda$ that optimises the cross-validation score. It could be thought of as a proxy for the validity of Laplace's equation. Intuitively the higher $\lambda$ is the more the Laplacian penalty is emphasised, and so the more likely the model is to be valid. 


\smallskip

The computational aspects of finding $\hat{u}(x,y)$ are identical to the case with one dimensional penalised regression. We will be able to express the penalty as a quadratic form on the coefficients of $\hat{u}(x,y)$ and then derived a penalised regression problem as before.  Define an inner product on our functions by

\[
	\langle f, g \rangle = \int_{\Omega} f(\mathbf{x})g(\mathbf{x})\mathbf{dx}.
\]

Then the laplacian penalty can be written as

\[
	\int_{\Omega} |\Delta f(\mathbf{x})|^2 \mathbf{dx} = \langle \Delta f, \Delta f \rangle.
\]

If $f = \sum c_i\phi_i$ then $\Delta f = \sum c_i \Delta \phi_i$. In the same manner as the previous roughness penalty, we can see that if f is a basis expansion then we can write the penalty as a polynomial in the coefficients

\[
	\langle \Delta f, \Delta f \rangle = \sum_{i=1}^m\sum_{j=1}^m c_ic_j \langle \Delta \phi_i, \Delta \phi_j \rangle.
\]

This can of course be written as $\mathbf{c'Kc}$ where $\mathbf{K}_{ij} = \langle \Delta \phi_i, \Delta \phi_j \rangle.$

\smallskip

We define the matrix $\mathbf{\Phi}$ as before with $\mathbf{\Phi}_{ij} = \phi_j(\mathbf{x}_i)$, $\mathbf{c} = (c_1, \dots, c_m)$. We can then write the penalty in terms of the coefficients

\[
	PENSSE(c) = (\mathbf{y - \Phi c})'(\mathbf{y - \Phi c}) + \lambda \mathbf{c'Kc}.
\]
\qedsymbol
\end{example}


\section{Non Linear Penalties}

Here we discuss generalisations of the roughess penalty to penaltys on nonlinear operators applied to the function. The problem is finding nice expressions for penalies of the form $\int_D |Tf(t)|^2 \mathrm{d}t$, where $T$ is not necessarily a linear operator anymore. Continuing with the case where we are working on the span of a finite set of basis functions, so an arbitary function $f$ can be written as $f(t) = \mathbf{c'\phi}(t)$, it will be shown how in some cases that the penalty can be written in the form $\mathbf{F(c)'KF(c)}$ where $F(c)$ is a vector valued function of our parameter vector $\mathbf{c}.$ In all the cases we have seen so far $\mathbf{F(\cdot)}$ was simply the identity function $\mathbf{F(c)} = \mathbf{c}.$ In the nonlinear case things can get more interesting as expected.

\begin{example}($f'' = f^2$) This example is purely artificial and intended to illustrate how the expresssion for the penalty term can be derived; it is a simple nonlinear ODE that is amenable to this technique. In this case our penalty is 
\[
	\int_\mathit{D} (f''(t) - f(t)^2)^2\mathrm{dt}.
\]

It now time to represent the penalty in the form above. The trick is that if our function is of the form $f = \sum c_i\phi_i$ then we will see that $Tf = f'' - f = \sum b_i\psi_i,$ for some other functions $\{\psi_i\}$. The coefficients $b_i$ depend on the original coefficients $c_i$, allowing the expression to be written as $Tf = \sum b_i(\mathbf{c})\psi_i.$ The nonlinear operator $T = f'' - f^2$ has the property that it maps from one vector space to another, the span of the $\phi_i$ functions to the span of the $\psi_i$ functions. The nonlinearity is reflected in the fact the coefficients in front of the $\psi_i$ depend nonlinearly on the coefficients of the $\phi_i$ functions.

\smallskip 

Since the image or range of the span of the $\phi_i$ under $T$ is itself a vector space and it is subset of the inner product space $\mathit{L}^2[D]$

\[
	f''(t) = \sum_{i=0}^mc_i\phi_i(t)''
\]

\[
	f(t)^2 = \sum_{i=0}^m\sum_{j=0}^mc_ic_j\phi_i(t)\phi_j(t).
\]

Hence,

\[
	f''(t) - f(t)^2 = \sum_{i=0}^mc_i\phi(t)'' - \sum_{i=0}^m\sum_{j=0}^mc_ic_j\phi_i(t)\phi_j(t).
\]

Notice the second term is a two dimensional, finite sum. We now need to find norm. As before we can write this as an inner product: $\langle f'' - f^2, f'' - f^2 \rangle$. The above expression is in the form $a_1\phi_1'' + \dots + a_m\phi_m'' +  b_{11}\phi_1\phi_1 + \dots b_{mm}\phi_m\phi_m$. This is a linear combination of a finite set of functions. So we can represent it as a quadratic form. The form above is a little awkward to work with. We would like to have it vary with one index only, or combine the $\phi_i'''$ and the $\phi_i\phi_j$ together. We would define a function $\pi(n)$ that returns the appropriate function either some $\phi_i$ or $\phi_i\phi_j$ depending on the value. Defining such a function is tricky however. One function is as follows

\[
	\pi(k) = \begin{cases}
		\phi_k'' & \text{if } k \leq m \\
		\phi_{(k-1)|m}\phi_{(k-1) \bmod m} & \text{if } k > m 
	\end{cases}
\]

Here $a|b$ is integer division, i.e. $a|b = \operatorname{floor}(a/b)$.

\smallskip

If we define $\psi_k(t)$ to be $\pi_k(t)$ and define a similar function $\sigma(i)$ for the coefficients we can define $f'' - f^2$ we get

\[
	f'' - f^2 = \sum \sigma(i)\psi_i.
\]

We now can express the penalty as a quadratic form

\[
	\int_\mathit{D}(f(t)'' - f(t)^2)^2 dt =  \mathbf{\sigma(c)}'\mathbf{K}\mathbf{\sigma(c}).
\]

Here $\mathbf{K}_{ij} = \langle \psi_i, \psi_j \rangle$ and $\mathbf{\sigma(c)} = (\sigma(1), \dots, \sigma(m + m^2)) = (c_1, \dots, c_m, c_1c_1, \dots, c_m c_m)$. The inner product is $\langle f, g \rangle = \int_\mathit{D} f(t)g(t)dt$. Since $\mathbf{\sigma(c)}$ is a second order polynomial in the $c_i$, the penalty is actually as fourth order polynomial in the $c_i$.

\smallskip

We can write the Penalised Sum of Squared Errors in terms of the $c_i$

\[
	PENSSE(\mathbf{c}) = \mathbf{(y - \Phi c)}'\mathbf{(y - \Phi c)} + \lambda \mathbf{\sigma(c)'K\sigma(c)}.
\]

\subsection*{Alternative Representation Of The Penalty By Splitting the Inner Product}

It is difficult to deal with the indexing above. An alternative is to break the function into two parts $f = f' + f^2$ where $f' = \sum c_k \phi_k'$ and $f^2 = \sum c_{k}c_{l}\phi_{k}\phi_{l}.$ Since $\langle f', f^2 \rangle = \langle f^2, f'$. We can represent the penalty in the form of the sum of three parts, letting $\mathbf{b} = (c_1c_1, c_1c_2, \dots)$

\[
	\langle f' + f^2, f' + f^2 \rangle = \mathbf{c'Kc} + 2\mathbf{c'Lb} + \mathbf{b'Mb}.
\]

Here $\mathbf{K}_{ij} = \langle \phi_i', \phi_j' \rangle.$ $\mathbf{L}$ and $\mathbf{M}$ similarly represent $\langle f', f^2 \rangle$ and $\langle f^2, f^2 \rangle$. 


\subsection*{Using the $\operatorname{Vec}$ Operator to Represent the Penalty}

\begin{definition}The $\operatorname{Vec}$ operator applied to a matrix stacks all the matrix's columns on top of each other \end{definition}

We can represent the vector of products $c_ic_j$ as $\operatorname{Vec}\mathbf{cc'}$. Note that each term $c_ic_j$ can appear twice if $i \neq j$. We must therefore scale any inner product matrices appropriately. We can write the penalty without messing with indices

\[
	\langle f' + f^2, f' + f^2 \rangle = \mathbf{c'Kc} +2\mathbf{c'L}\operatorname{Vec} (\mathbf{cc'}) + \operatorname{Vec}(\mathbf{cc'})'\mathbf{M}\operatorname{Vec}(\mathbf{cc'})
\]

\qedsymbol
\end{example}

\section{Systems Of Ordinary Differential Equations (ODES)}

The approach generalises quite well to systems of differential equations. In contrast to the multivariate case above, where one output variable depended on multiple input variables, we now have a time series of vectors $\mathbf{x}_k$ depending only on time. We give each component of $\mathbf{x}(t)$ its own basis expansion, but keep the same basis functions. For this section we will concentrate on the two dimensional case, except where it is obvious that we are considering an arbitrary number of dimensions.

\begin{align*}
	\mathbf{x}_i(t) &= \mathbf{c}_i'\mathbf{\phi}(t) \\
			&= c_{i1}\phi_i(t) + \dots + c_{im}{\phi_m(t)}.
\end{align*}

\subsection*{Least Squares Penalty}

We will also need an inner product of some sort to define the least squares and roughness penalties. It seems reasonable to use the standard dot product $\mathbf{x} \cdot \mathbf{x}$ in place of $|x|^2$ for our penalties. For a two dimensional series $\mathbf{x} = (x,y)$ with coefficient vectors $\mathbf{b}$ and $\mathbf{c}$, our least squares penalty has the form

\begin{align*}
	SSE &= \sum_{i=1}^n\{ [x_i - \sum_{j=1}^mb_j\phi_j(t_i)]^2 + [y_i - \sum_{j=1}^mc_j\phi_j(t_i)]^2 \} \\
	    &= (\mathbf{x} - \mathbf{\Phi}\mathbf{b})'(\mathbf{x} - \mathbf{\Phi}\mathbf{b}) + (\mathbf{y} - \mathbf{\Phi}\mathbf{c})'(\mathbf{y} - \mathbf{\Phi}\mathbf{c})\\
	    &= \|\mathbf{[x \: y] - \Phi[b \: c]}\|_F^2.
\end{align*}

$\|\cdot\|^2_F$ is the Frobenius Norm. For an $n \times m$ matrix it is defined in terms of the sums of the squares of its elements

\[
	\|A\|_F^2 = \sum_{i = 1}^n\sum_{j=1}^m A_{ij}^2.
\]

\subsection*{Differential Equation Penalty}

A system of differential equations has the form

\begin{align*}
	\mathbf{x}' &= f(\mathbf{x}, t) \\
	 \mathbf{x}(t_0) &= \mathbf{x_0}  
\end{align*}

We won't be too worried about making sure our penalties are in the standard form above though. It sometimes convenient to leave a higher order derivative on the left hand side instead of converting them to the standard form. 

\smallskip

As usual we will be dealing with expressions of the form $\|T\mathbf{x}\|,$ except $\mathbf{x}(t)$ is a vector valued function, or a curve. We will use the following $L^2$ inner product to induce our norm

\[
	\langle \mathbf{x}, \mathbf{y} \rangle = \int_D \mathbf{x}(t) \cdot \mathbf{y}(t) \mathrm{d}t.
\]

Thanks to the relative abstractness of an inner product space, we will not have to make too many changes in moving into multivariate data. 

\smallskip

\begin{example} Roughness Penalties \end{example}

Suppose we have a penalty of the form $\|\mathbf{x}''\|$. If we expand out the inner product we see $\|\mathbf{x}''\|^2 = \sum \int_\mathit{D} \mathbf{x}_k(t)^2\mathrm{d}t.$ Since we assume each component of $\mathbf{x}$ has its own basis function expansion, we can use the previous results on roughness penalties to find
\[
	\|\mathbf{x}\|^2 = \sum \mathbf{c}_i' \mathbf{K} \mathbf{c}_i.
\]

Here as usual, $\mathbf{K}_{ij} = \langle \phi_i, \phi_j \rangle$ where we use the one-dimensional inner product $\langle f, g \rangle = \int_D fg \mathrm{d}t.$

\smallskip

Notice that our penalty decomposes into a sum of simpler penalties. This suggests we could take a multiple penalty approach. Instead of an expression of the form $\lambda\|\mathbf{x}\|^2$ we have a sum of penalties

\[
	\sum \lambda_k \|\mathbf{x}_k\|^2  =  \lambda_1 \mathbf{c}_1' \mathbf{K} \mathbf{c}_1 + \dots + \lambda_m \mathbf{c}_m' \mathbf{K} \mathbf{c}_m
\]

\begin{example} Generalised Roughness Penalties \end{example}

In all the previous cases the weights $\lambda$ have been \emph{external} to the norms we used. What if we instead used a weighted inner product of the form $\langle x, y \rangle_Q = \mathbf{x'Qy}?$ To define an inner product we must have that $\mathbf{Q}$ be symmetric and positive definite. However since we only have $\lambda \geq 0$ in general, we will only say that $\mathbf{Q}$ must be positive semidefinite and symmetric. We define a symmetric, positive semidefinite bilinear form, but still retain the inner product notation, $\langle \mathbf{x}, \mathbf{y} \rangle = \int_D \mathbf{x'Qy}\mathrm{d}t.$

\smallskip

What will the roughness penalty look like with this change? If $\mathbf{Q}$ is diagonal then we will get the results above with the multiple $\lambda$'s.

\smallskip

In the case of a general suitable matrix, by making use of the usual approach  we find

\begin{align*}
	\|\mathbf{x}''\|_Q^2 &= \sum \sum q_{ij} \langle \mathbf{x}_i'' , \mathbf{x}_j'' \rangle_{L^2} \\
			   &= \sum \sum q_{ij} \mathbf{c}_i' \mathbf{K} \mathbf{c}_j.
\end{align*}


\begin{example} Lotka Volterra Equations \end{example}

The Lotka Volterra Equations are a model of the interactions of a prey and predator population. They are as follows

\begin{align*}
	x' &= x(\alpha - \beta y) \\ 
	   &= \alpha x - \beta xy \\
	y' &= -y(\gamma - \delta x) \\ 
	   &= -\gamma y - \delta xy.  
\end{align*}

We will be using the standard dot product for our norms. In the standard inner product, the different terms are independent of each other; we only need to look at one equation , so without loss of generality we will find a formula for 

\[
	\|x' - \alpha x - \beta xy\|^2.
\]

Here we are computing a ``one dimensional'' penalty. Notice we no longer assume anything about the signs of the coefficients.

\smallskip

By the usual methods we see

\begin{align*}
	\|x' - \alpha x - \beta xy\|^2 &= \langle x' - \alpha x - \beta xy, x' - \alpha x - \beta xy \rangle \\
				      &= \|x'\|^2 + \alpha^2 \|x\|^2 + \beta^2 \|xy\|^2 + 2\alpha\beta\langle xy, x\rangle - 2\alpha \langle x', x\rangle - 2\beta \langle x', xy \rangle.
\end{align*}

Many of the terms were covered in previous examples, so only examine the nonlinear interaction terms bear any serious examination. We will not bother defining the matrices that appear, as their definitions should be clear by now. 

\smallskip

Firstly $ xy = \sum \sum b_i \phi_i c_j \phi_j.$ If we define $d = (b_1c_1, \dots, b_nc_n)$ we get

\[
	\|xy\|^2 = \mathbf{d'Kd}.
\]

Where $\mathbf{K}_{ikjl} = \langle \phi_i\phi_k, \phi_j\phi_l \rangle.$ We can devise similar results for all the other terms. 

\smallskip

We will use the $\operatorname{Vec}$ operator again to save space. Let $\mathbf{d} = \operatorname{Vec}(\mathbf{cb'})$ as before. We can express the penalty analytically

\[
	\|x' - \alpha x - \beta xy\|^2 = \mathbf{b'Kb} + \alpha^2 \mathbf{b'Lb} + \beta^2 \mathbf{d'M'd} + 2\alpha\beta \mathbf{d'Nb} - 2\alpha\mathbf{b'Ob} - 2\beta\mathbf{b'Pd}.
\]
\end{document}
